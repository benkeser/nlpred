<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>
<code>nlpred</code>: Small-sample optimized estimators of nonlinear risks • nlpred</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="&lt;code&gt;nlpred&lt;/code&gt;: Small-sample optimized estimators of nonlinear risks">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">nlpred</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">1.0.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/using_nlpred.html">`nlpred`: Small-sample optimized estimators of nonlinear risks</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>
<code>nlpred</code>: Small-sample optimized estimators of nonlinear risks</h1>
                        <h4 class="author">David Benkeser</h4>
            
            <h4 class="date">12 September, 2019</h4>
      
      
      <div class="hidden name"><code>using_nlpred.Rmd</code></div>

    </div>

    
    
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>When predicting an outcome is the scientific goal, one must decide on a metric by which to evaluate the quality of predictions. Often, the performance of a prediction algorithm must be estimated using the same data that are used to train the algorithm. To correct for overly optimistic assessments of performance, cross-validation is often used. However, standard <span class="math inline">\(K\)</span>-fold cross-validated risk estimates may perform poorly in small samples when one considers <em>nonlinear</em> risks. This includes several popular risk criteria including the area under the ROC operating characteristics curve (AUC). The <code>nlpred</code> package implements several estimators that are tailored for improved cross-validated estimation of nonlinear risks. This vignette provides a brief overview of the motivation for these estimators, and demonstrations of how they are implemented in the <code>nlpred</code> package.</p>
</div>
<div id="motivation" class="section level1">
<h1 class="hasAnchor">
<a href="#motivation" class="anchor"></a>Motivation</h1>
<p>Prediction is important in many areas of research. Modern technology has led to collection of vast amounts of data; for example in biomedical studies, we now routinely collect genetic sequences, gene expressions, proteomics, and metabolomics. Relative to the amount of information measured on each unit, the total number of units available may be quite modest. Many practical applications thus require methodology that enables researchers to simultaneously develop and evaluate performance of prediction algorithms in small samples. It is well known that estimating performance of an algorithm using the same data that were used to train the algorithm often leads to an overly optimistic estimate of performance. To correct for this, it is common to use <span class="math inline">\(K\)</span>-fold cross-validation (CV). <span class="math inline">\(K\)</span>-fold CV generalizes partitions the data into several distinct groups. The prediction algorithm is developed using all but one group, and the prediction metric is estimated in the remaining group. This is repeated until each partition has been used to estimate the risk once. The <span class="math inline">\(K\)</span>-fold CV risk estimate is the average of these partition-specific estimates.</p>
<p>Theoretical frameworks have been developed for estimation of <span class="math inline">\(K\)</span>-fold CV risks that apply to arbitrary learning algorithms <span class="citation">(Hubbard, Kherad-Pajouh, and Laan 2016)</span>. Moreover, it is often possible to construct closed-form, computationally efficient confidence intervals and hypothesis tests based on <span class="math inline">\(K\)</span>-fold CV estimates, e.g., <span class="citation">(LeDell, Petersen, and Laan 2015)</span>. However, these estimators can suffer from poor behavior for certain risks. In particular, risks that are <em>non-linear</em> in the data generating distribution may suffer from poor performance. Whereas <em>linear</em> metrics can be estimated using estimators that themselves are linear (i.e., behave like means), non-linear metrics typically require <em>asymptotically linear</em> estimators. Such estimators behave as a mean <em>plus a remainder term</em>. While the remainder is generally negligible in large samples, it may contribute substantially to the behavior of the estimator in small samples.</p>
<p>In our recent work (references to be added when available), we have developed improved estimators of nonlinear risk functions. Our approach involves viewing the risk criteria as a statistical function of certain key parameters the data generating distribution. Standard CV estimators use the validation data to estimate these parameters, for example, using nonparametric maximum likelihood. However, as discussed above, this is problematic when validation samples are small. Thus, our proposal uses the training sample twice: once to train the prediction algorithm, and then again to estimate the relevant parameters of the data generating distribution that are needed to evaluate the risk criteria of interest. Because of the double-usage of the data, these estimates may exhibit bias. This motivates some form of bias correction. <code>nlpred</code> implements three asymptotically equivalent approaches: CV targeted minimum loss estimation (CVTMLE), one-step estimation, and estimating equations. We refer readers to our publication for further details on how the estimators are constructed.</p>
</div>
<div id="area-under-the-receiver-operating-characteristics-roc-curve" class="section level1">
<h1 class="hasAnchor">
<a href="#area-under-the-receiver-operating-characteristics-roc-curve" class="anchor"></a>Area under the receiver operating characteristics (ROC) curve</h1>
<p>The area under the ROC curve (hence, AUC) is a popular risk criteria for evaluating prediction algorithms. Suppose we have a prediction algorithm <span class="math inline">\(\psi\)</span> that maps a feature vector <span class="math inline">\(X\)</span> into a predicted probability of a binary outcome <span class="math inline">\(Y\)</span>. The AUC of <span class="math inline">\(\psi\)</span> can be defined as follows. Consider drawing <span class="math inline">\((X_1, Y_1)\)</span> at random from the population of units with <span class="math inline">\(Y = 1\)</span> and <span class="math inline">\((X_2, Y_2)\)</span> independently from the population of units with <span class="math inline">\(Y = 0\)</span>. The AUC can be interpreted as <span class="math inline">\(P(\psi(X_1) &gt; \psi(X_2) | Y_1 = 1, Y_2 = 0)\)</span>. That is, the probability that the predicted risk of <span class="math inline">\(X_1\)</span> is higher than that of <span class="math inline">\(X_2\)</span>.</p>
<p>Estimates of CV AUC can be implemented in <code>nlpred</code> as follows.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># load package</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/library">library</a></span>(nlpred)</a></code></pre></div>
<pre><code>## Loading required package: data.table</code></pre>
<pre><code>## Registered S3 methods overwritten by 'ggplot2':
##   method         from 
##   [.quosures     rlang
##   c.quosures     rlang
##   print.quosures rlang</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="co"># turn off messages from np package</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/options">options</a></span>(<span class="dt">np.messages=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb4-3" data-line-number="3"></a>
<a class="sourceLine" id="cb4-4" data-line-number="4"><span class="co"># simulate data</span></a>
<a class="sourceLine" id="cb4-5" data-line-number="5">n &lt;-<span class="st"> </span><span class="dv">200</span></a>
<a class="sourceLine" id="cb4-6" data-line-number="6">p &lt;-<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb4-7" data-line-number="7">X &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/data.frame">data.frame</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/matrix">matrix</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/stats/topics/Normal">rnorm</a></span>(n<span class="op">*</span>p), <span class="dt">nrow =</span> n, <span class="dt">ncol =</span> p))</a>
<a class="sourceLine" id="cb4-8" data-line-number="8">Y &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/stats/topics/Binomial">rbinom</a></span>(n, <span class="dv">1</span>, <span class="kw"><a href="https://www.rdocumentation.org/packages/stats/topics/Logistic">plogis</a></span>(X[,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>X[,<span class="dv">10</span>]))</a>
<a class="sourceLine" id="cb4-9" data-line-number="9"></a>
<a class="sourceLine" id="cb4-10" data-line-number="10"><span class="co"># get cv auc estimates for logistic regression</span></a>
<a class="sourceLine" id="cb4-11" data-line-number="11">logistic_cv_auc_ests &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cv_auc.html">cv_auc</a></span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">K =</span> <span class="dv">5</span>, </a>
<a class="sourceLine" id="cb4-12" data-line-number="12">                               <span class="dt">learner =</span> <span class="st">"glm_wrapper"</span>,</a>
<a class="sourceLine" id="cb4-13" data-line-number="13">                               <span class="dt">nested_cv =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb4-14" data-line-number="14">logistic_cv_auc_ests</a></code></pre></div>
<pre><code>##                est         se       cil       ciu
## cvtmle   0.8624375 0.02697991 0.8095579 0.9153172
## onestep  0.8622338 0.02519591 0.8128507 0.9116169
## esteq    0.8757339 0.02519591 0.8263508 0.9251170
## standard 0.8618722 0.02616729 0.8105852 0.9131591</code></pre>
<p>The main options to the <code>cv_auc</code> function are <code>Y</code>, the outcome, <code>X</code>, the features, and <code>K</code>, the number of cross-validation folds. The <code>learner</code> option specifies the learning algorithm of interest. See the <strong>Writing wrappers</strong> section below for more details. For now, it suffices to say that <code>"glm_wrapper"</code> corresponds to a main effects logistic regression. The <code>nested_cv</code> option is an important option in the estimation procedure. It specifies whether to use nested CV to estimate the nuisance parameters in each training sample. Obviously, requiring nested cross-validation adds considerable computational expense, so it is natural to inquire as to when this is necessary. In general, we recommend nested CV for more aggressive learning algorithms. Because logistic regression is a fairly stable algorithm, it may not be necessary in this case.</p>
<p>The printed output shows four estimates of CV AUC based on the three different bias corrections (<code>cvtmle</code>, <code>onestep</code>, <code>esteq</code>) as well as the standard CV AUC estimate (<code>empirical</code>) <span class="citation">(LeDell, Petersen, and Laan 2015)</span>. Also shown is the influence function-based standard error estimate (<code>se</code>) and the limits of a, by default, 95% confidence interval. The level of the confidence interval is controlled by the <code>ci_level</code> of the <code>print.cv_auc</code> method.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="co"># print a 90% CI</span></a>
<a class="sourceLine" id="cb6-2" data-line-number="2"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/print">print</a></span>(logistic_cv_auc_ests, <span class="dt">ci_level =</span> <span class="fl">0.9</span>)</a></code></pre></div>
<pre><code>##                est         se       cil       ciu
## cvtmle   0.8624375 0.02697991 0.8180595 0.9068155
## onestep  0.8622338 0.02519591 0.8207902 0.9036774
## esteq    0.8757339 0.02519591 0.8342903 0.9171775
## standard 0.8618722 0.02616729 0.8188308 0.9049135</code></pre>
<p>We now consider a more aggressive algorithm: random forests (as implemented in the <code>ranger</code> package).</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="co"># load the ranger package</span></a>
<a class="sourceLine" id="cb8-2" data-line-number="2"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/library">library</a></span>(ranger)</a>
<a class="sourceLine" id="cb8-3" data-line-number="3"></a>
<a class="sourceLine" id="cb8-4" data-line-number="4"><span class="co"># set a seed (reason to be made clear)</span></a>
<a class="sourceLine" id="cb8-5" data-line-number="5"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/Random">set.seed</a></span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb8-6" data-line-number="6"></a>
<a class="sourceLine" id="cb8-7" data-line-number="7"><span class="co"># get cv auc estimates for random forest</span></a>
<a class="sourceLine" id="cb8-8" data-line-number="8"><span class="co"># using nested cross-validation for nuisance parameter estimation</span></a>
<a class="sourceLine" id="cb8-9" data-line-number="9">rf_cv_auc_ests &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cv_auc.html">cv_auc</a></span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">K =</span> <span class="dv">5</span>, </a>
<a class="sourceLine" id="cb8-10" data-line-number="10">                         <span class="dt">learner =</span> <span class="st">"ranger_wrapper"</span>, </a>
<a class="sourceLine" id="cb8-11" data-line-number="11">                         <span class="dt">nested_cv =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb8-12" data-line-number="12">rf_cv_auc_ests</a></code></pre></div>
<pre><code>##                est         se       cil       ciu
## cvtmle   0.8194392 0.03182629 0.7570608 0.8818175
## onestep  0.8190492 0.03134540 0.7576133 0.8804851
## esteq    0.8252113 0.03134540 0.7637754 0.8866471
## standard 0.8306266 0.02956253 0.7726851 0.8885681</code></pre>
<p>By default, <code>cv_auc</code> will use <code>K-1</code> folds for the nested cross-validation. This choice is a sensible default since it allows for considerably less learner training relative to, e.g., two layers of <span class="math inline">\(K\)</span>-fold CV, because certain learners can be re-used across different partitions of the data. However, if one wishes to control this, there is the <code>nested_K</code> option.</p>
</div>
<div id="available-wrappers" class="section level1">
<h1 class="hasAnchor">
<a href="#available-wrappers" class="anchor"></a>Available wrappers</h1>
<p>The table below shows wrappers for learners that are available in <code>nlpred</code>.</p>
<table class="table">
<thead><tr class="header">
<th>Wrapper</th>
<th>Description</th>
<th>Reference</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><code>glm_wrapper</code></td>
<td>logistic regression</td>
<td></td>
</tr>
<tr class="even">
<td><code>stepglm_wrapper</code></td>
<td>stepwise logistic regression</td>
<td></td>
</tr>
<tr class="odd">
<td><code>xgboost_wrapper</code></td>
<td>eXtreme gradient boosting</td>
<td><span class="citation">Chen and Guestrin (2016)</span></td>
</tr>
<tr class="even">
<td><code>ranger_wrapper</code></td>
<td>random forests</td>
<td><span class="citation">Wright and Ziegler (2017)</span></td>
</tr>
<tr class="odd">
<td><code>randomforest_wrapper</code></td>
<td>random forests</td>
<td><span class="citation">Liaw and Wiener (2002)</span></td>
</tr>
<tr class="even">
<td><code>superlearner_wrapper</code></td>
<td>super learner</td>
<td><span class="citation">Polley et al. (2019)</span></td>
</tr>
<tr class="odd">
<td><code>glmnet_wrapper</code></td>
<td>elastic net regression</td>
<td><span class="citation">Simon et al. (2011)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="writing-wrappers" class="section level1">
<h1 class="hasAnchor">
<a href="#writing-wrappers" class="anchor"></a>Writing wrappers</h1>
<p>It is not difficult to write your own function that is compatible with <code>cv_auc</code> and other functions in the <code>nlpred</code> package. Let’s examine the <code>glm_wrapper</code> function.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1">glm_wrapper</a></code></pre></div>
<pre><code>## function (train, test) 
## {
##     if (!is.data.frame(train$X)) {
##         train$X &lt;- data.frame(train$X)
##     }
##     if (!is.data.frame(test$X)) {
##         test$X &lt;- data.frame(test$X)
##     }
##     glm_fit &lt;- stats::glm(train$Y ~ ., data = train$X, family = stats::binomial())
##     train_pred &lt;- stats::predict(glm_fit, newdata = train$X, 
##         type = "response")
##     test_pred &lt;- stats::predict(glm_fit, newdata = test$X, type = "response")
##     return(list(test_pred = test_pred, train_pred = train_pred, 
##         model = NULL, train_y = train$Y, test_y = test$Y))
## }
## &lt;bytecode: 0x7fae39629778&gt;
## &lt;environment: namespace:nlpred&gt;</code></pre>
<p>We see that the function expect input in the form of two lists: <code>train</code> and <code>test</code>. In these lists, we expect to find entries <code>X</code> and <code>Y</code> corresponding to the features and outcomes, respectively, in the a giving training sample and validation/testing sample. The salient points of the workflow of this function are: fit a main terms logistic regression and store it in the <code>glm_fit</code> object; obtain predictions on both the training data and testing data; structure the output to have a particular format. In particular, the output should be a list with named entries <code>test_pred</code>, predictions in the test sample, <code>train_pred</code>, predictions in the training sample, <code>model</code>, the fitted model (optional; only needed if you wish to examine it in the <code>$prediction_list</code> entry of the output of <code>cv_auc</code>), <code>train_y</code>, the training sample outcomes (copied from <code>train$Y</code>), and <code>test_y</code>, the testing sample outcomes (copied from <code>test$Y</code>).</p>
</div>
<div id="sensitivity-constrained-rate-of-negative-prediction-scrnp" class="section level1">
<h1 class="hasAnchor">
<a href="#sensitivity-constrained-rate-of-negative-prediction-scrnp" class="anchor"></a>Sensitivity constrained rate of negative prediction (SCRNP)</h1>
<p>The sensitivity constrained rate of negative prediction (SCRNP) can be described as follows. Suppose again that we have a prediction function <span class="math inline">\(\psi\)</span> that maps features <span class="math inline">\(X\)</span> into a predicted probability of a binary outcome <span class="math inline">\(Y\)</span>. Suppose we choose a cutoff <span class="math inline">\(c_0\)</span>, such that <span class="math inline">\(P(\psi(X) &gt; c_0 | Y = 1) \ge \rho\)</span> for a user-defined <span class="math inline">\(\rho\)</span>. That is, we enforce that the sensitivity of a classifier based on <span class="math inline">\(\psi\)</span> is at least <span class="math inline">\(\rho\)</span>. The SCRNP is then <span class="math inline">\(P(\psi(X) \le c_0)\)</span>; that is, the proportion of all data units that would be classified as a “control” (i.e., <span class="math inline">\(Y = 0\)</span>).</p>
<p>To motivate SCRNP, consider developing a prediction algorithm for breast cancer incidence in women. We would like to identify a large proportion of women who will eventually develop breast cancer; that is, we would like to enforce that our procedure for classifying women at high-risk has high sensitivity. However, women with high predicted risk of cancer may be recommended to undergo more invasive screening procedures. So beyond our sensitivity constraint, we would like to maximize the proportion of women who are not required to undergo additional screening. The SCRNP describes this exactly this proportion. <span class="citation">Zheng et al. (2018)</span> discuss SCRNP in the context of HIV prevention.</p>
<p>Estimating SCRNP using traditional <span class="math inline">\(K\)</span>-fold CV approaches is particularly challenging because we need to estimate a quantile of the distribution of <span class="math inline">\(\psi(X)\)</span> amongst those with <span class="math inline">\(Y = 1\)</span>. If there are few observations with <span class="math inline">\(Y = 1\)</span> in the validation fold, then this estimation will be highly unstable and will cause downstream instability in the estimate of CV SCRNP. This makes our approach particularly appealing for this problem.</p>
<p>The syntax to estimate CV SCRNP is through the <code>cv_scrnp</code> function as shown below.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="co"># get cv scrnp estimates for logistic regression</span></a>
<a class="sourceLine" id="cb12-2" data-line-number="2">logistic_cv_scrnp_ests &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cv_scrnp.html">cv_scrnp</a></span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">K =</span> <span class="dv">5</span>, </a>
<a class="sourceLine" id="cb12-3" data-line-number="3">                               <span class="dt">learner =</span> <span class="st">"glm_wrapper"</span>,</a>
<a class="sourceLine" id="cb12-4" data-line-number="4">                               <span class="dt">nested_cv =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb12-5" data-line-number="5">logistic_cv_scrnp_ests</a></code></pre></div>
<pre><code>##                est         se       cil       ciu
## cvtmle   0.2793509 0.06083550 0.1601155 0.3985863
## onestep  0.1667606 0.06263884 0.0439907 0.2895305
## esteq    0.1667606 0.06263884 0.0439907 0.2895305
## standard 0.2344727 0.03846303 0.1590865 0.3098588</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="co"># get cv scrnp estimates for random forest</span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2"><span class="co"># using nested cross-validation for nuisance parameter estimation</span></a>
<a class="sourceLine" id="cb14-3" data-line-number="3">rf_cv_scrnp_ests &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cv_scrnp.html">cv_scrnp</a></span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">K =</span> <span class="dv">5</span>, </a>
<a class="sourceLine" id="cb14-4" data-line-number="4">                         <span class="dt">learner =</span> <span class="st">"ranger_wrapper"</span>, </a>
<a class="sourceLine" id="cb14-5" data-line-number="5">                         <span class="dt">nested_cv =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb14-6" data-line-number="6">rf_cv_scrnp_ests</a></code></pre></div>
<pre><code>##                est         se       cil       ciu
## cvtmle   0.2235223 0.05297417 0.1196949 0.3273498
## onestep  0.1917946 0.05355483 0.0868291 0.2967602
## esteq    0.1917946 0.05355483 0.0868291 0.2967602
## standard 0.2149332 0.05479353 0.1075399 0.3223265</code></pre>
</div>
<div id="other-methods-implemented-in-nlpred" class="section level1">
<h1 class="hasAnchor">
<a href="#other-methods-implemented-in-nlpred" class="anchor"></a>Other methods implemented in <code>nlpred</code>
</h1>
<p>To compare the novel methods in <code>nlpred</code> to existing approaches, we have included several alternative approaches to estimating performance of these quantities.</p>
<div id="bootstrap-corrections" class="section level3">
<h3 class="hasAnchor">
<a href="#bootstrap-corrections" class="anchor"></a>Bootstrap corrections</h3>
<p>The functions <code>boot_auc</code> and <code>boot_scrnp</code> can be used to estimate bootstrap based performance of prediction algorithms. There are several available approaches. In particular, each function implements a standard bootstrap correction <span class="citation">(Harrell, Lee, and Mark 1996)</span>, as well as an 0.632 correction described in Chapter 7 of <span class="citation">Friedman, Hastie, and Tibshirani (2001)</span>.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="co"># get bootstrap estimated auc of logistic regression</span></a>
<a class="sourceLine" id="cb16-2" data-line-number="2">boot_auc_est &lt;-<span class="st"> </span><span class="kw"><a href="../reference/boot_auc.html">boot_auc</a></span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">learner =</span> <span class="st">"glm_wrapper"</span>, </a>
<a class="sourceLine" id="cb16-3" data-line-number="3">                         <span class="dt">correct632 =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb16-4" data-line-number="4">boot_auc_est</a></code></pre></div>
<pre><code>## $auc
## [1] 0.8616508
## 
## $n_valid_boot
## [1] 500</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="co"># with 0.632 correction </span></a>
<a class="sourceLine" id="cb18-2" data-line-number="2">boot632_auc_est &lt;-<span class="st"> </span><span class="kw"><a href="../reference/boot_auc.html">boot_auc</a></span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">learner =</span> <span class="st">"glm_wrapper"</span>, </a>
<a class="sourceLine" id="cb18-3" data-line-number="3">                         <span class="dt">correct632 =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb18-4" data-line-number="4">boot632_auc_est</a></code></pre></div>
<pre><code>## $auc
## [1] 0.8598091
## 
## $n_valid_boot
## [1] 500</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="co"># get bootstrap estimated scrnp of logistic regression</span></a>
<a class="sourceLine" id="cb20-2" data-line-number="2">boot_scrnp_est &lt;-<span class="st"> </span><span class="kw"><a href="../reference/boot_scrnp.html">boot_scrnp</a></span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">learner =</span> <span class="st">"glm_wrapper"</span>, </a>
<a class="sourceLine" id="cb20-3" data-line-number="3">                             <span class="dt">correct632 =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb20-4" data-line-number="4">boot_scrnp_est</a></code></pre></div>
<pre><code>## $scrnp
## [1] 0.240575
## 
## $n_valid_boot
## [1] 200</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1"><span class="co"># with 0.632 correction </span></a>
<a class="sourceLine" id="cb22-2" data-line-number="2">boot632_scrnp_est &lt;-<span class="st"> </span><span class="kw"><a href="../reference/boot_scrnp.html">boot_scrnp</a></span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">learner =</span> <span class="st">"glm_wrapper"</span>, </a>
<a class="sourceLine" id="cb22-3" data-line-number="3">                         <span class="dt">correct632 =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb22-4" data-line-number="4">boot632_scrnp_est</a></code></pre></div>
<pre><code>## $scrnp
## [1] 0.07590848
## 
## $n_valid_boot
## [1] 200</code></pre>
</div>
<div id="leave-pairs-out-cv-auc-estimator" class="section level3">
<h3 class="hasAnchor">
<a href="#leave-pairs-out-cv-auc-estimator" class="anchor"></a>Leave-pairs-out CV AUC estimator</h3>
<p>Another proposal for estimating AUC is using leave-pairs-out CV <span class="citation">(Airola et al. 2011)</span>. In this approach, a random observation with <span class="math inline">\(Y = 0\)</span> and <span class="math inline">\(Y = 1\)</span> are left out; the algorithm is trained on the remaining data and predictions are made on the two held out observations. The estimate is the proportion of these pairs for which the <span class="math inline">\(Y = 1\)</span> observation had higher predicted risk than the <span class="math inline">\(Y = 0\)</span> observation. Because it can be quite computationally expensive to retrain algorithms for every, we include an option <code>max_pairs</code> to specify the maximum number of pairs to leave out. If left equal to <code>NULL</code>, then every possible case/control pair is left out in turn.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1"><span class="co"># leave out at most 250 pairs</span></a>
<a class="sourceLine" id="cb24-2" data-line-number="2">lpo_cv_auc_est &lt;-<span class="st"> </span><span class="kw"><a href="../reference/lpo_auc.html">lpo_auc</a></span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">learner =</span> <span class="st">"glm_wrapper"</span>,</a>
<a class="sourceLine" id="cb24-3" data-line-number="3">                          <span class="dt">max_pairs =</span> <span class="dv">250</span>)</a>
<a class="sourceLine" id="cb24-4" data-line-number="4">lpo_cv_auc_est</a></code></pre></div>
<pre><code>## $auc
## [1] 0.84</code></pre>
</div>
</div>
<div id="parallelization-in-nlpred" class="section level1">
<h1 class="hasAnchor">
<a href="#parallelization-in-nlpred" class="anchor"></a>Parallelization in <code>nlpred</code>
</h1>
<p>Unfortunately parallelization is not yet available in <code>nlpred</code>, but will be added as a feature soon.</p>
</div>
<div id="some-rules-of-thumb-based-on-simulation-studies" class="section level1">
<h1 class="hasAnchor">
<a href="#some-rules-of-thumb-based-on-simulation-studies" class="anchor"></a>Some rules of thumb based on simulation studies</h1>
<p>From extensive simulation studies, here are a few relevant observations.</p>
<ul>
<li>Any algorithm that is more complex than a standard logistic regression model would likely benefit from utilizing the nested cross-validation routine. Even for logistic regression, nested cross-validation does not tend to hurt performance too much.</li>
<li>Setting <code>K = 10</code> or <code>K = 20</code> tended to yield the best performance across a variety of settings.</li>
<li>Setting <code>inner_K = 5</code> tended to yield the best performance across a variety of settings.</li>
<li>The CVTMLE of CV SCRNP vastly outperformed the others.</li>
<li>The estimating equations estimator of CV AUC tended to outperform the others, though the difference was not drastic.</li>
<li>Confidence intervals tended to be anti-conservative in small samples. Nominal coverage can be expected at around 500 observations, though this is obviously extremely context dependent.</li>
<li>Bootstrap methods should only be used for simple learning algorithms such as logistic regression.</li>
<li>The 0.632-correction improves performance of bootstrap estimators, but is generally less efficient than our newer approaches.</li>
</ul>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-airola2011experimental">
<p>Airola, Antti, Tapio Pahikkala, Willem Waegeman, Bernard De Baets, and Tapio Salakoski. 2011. “An Experimental Comparison of Cross-Validation Techniques for Estimating the Area Under the ROC Curve.” <em>Computational Statistics &amp; Data Analysis</em> 55 (4). Elsevier: 1828–44. <a href="https://doi.org/10.1016/j.csda.2010.11.018" class="uri">https://doi.org/10.1016/j.csda.2010.11.018</a>.</p>
</div>
<div id="ref-chen2016xgboost">
<p>Chen, Tianqi, and Carlos Guestrin. 2016. “xgboost: A Scalable Tree Boosting System.” In <em>Proceedings of the 22nd SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 785–94. ACM. <a href="https://doi.org/10.1145/2939672.2939785" class="uri">https://doi.org/10.1145/2939672.2939785</a>.</p>
</div>
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. 10. Springer Series in Statistics New York, NY, USA. <a href="https://doi.org/10.1007/978-0-387-84858-7" class="uri">https://doi.org/10.1007/978-0-387-84858-7</a>.</p>
</div>
<div id="ref-harrell1996multivariable">
<p>Harrell, Frank E, Kerry L Lee, and Daniel B Mark. 1996. “Multivariable Prognostic Models: Issues in Developing Models, Evaluating Assumptions and Adequacy, and Measuring and Reducing Errors.” <em>Statistics in Medicine</em> 15 (4). Wiley Online Library: 361–87. <a href="https://doi.org/10.1002/(SICI)1097-0258(19960229)15:4&lt;361::AID-SIM168&gt;3.0.CO;2-4" class="uri">https://doi.org/10.1002/(SICI)1097-0258(19960229)15:4&lt;361::AID-SIM168&gt;3.0.CO;2-4</a>.</p>
</div>
<div id="ref-hubbard2016statistical">
<p>Hubbard, Alan E, Sara Kherad-Pajouh, and Mark J van der Laan. 2016. “Statistical Inference for Data Adaptive Target Parameters.” <em>The International Journal of Biostatistics</em> 12 (1). De Gruyter: 3–19. <a href="https://doi.org/10.1515/ijb-2015-0013" class="uri">https://doi.org/10.1515/ijb-2015-0013</a>.</p>
</div>
<div id="ref-ledell2015computationally">
<p>LeDell, Erin, Maya Petersen, and Mark J van der Laan. 2015. “Computationally Efficient Confidence Intervals for Cross-Validated Area Under the ROC Curve Estimates.” <em>Electronic Journal of Statistics</em> 9 (1). NIH Public Access: 1583. <a href="https://doi.org/10.1214/15-EJS1035" class="uri">https://doi.org/10.1214/15-EJS1035</a>.</p>
</div>
<div id="ref-randomForest_pkg">
<p>Liaw, Andy, and Matthew Wiener. 2002. “Classification and Regression by randomForest.” <em>R News</em> 2 (3): 18–22. <a href="https://CRAN.R-project.org/doc/Rnews/" class="uri">https://CRAN.R-project.org/doc/Rnews/</a>.</p>
</div>
<div id="ref-superlearner_pkg">
<p>Polley, Eric, Erin LeDell, Chris Kennedy, and Mark van der Laan. 2019. <em>SuperLearner: Super Learner Prediction</em>. <a href="https://CRAN.R-project.org/package=SuperLearner" class="uri">https://CRAN.R-project.org/package=SuperLearner</a>.</p>
</div>
<div id="ref-glmnet_pkg">
<p>Simon, Noah, Jerome Friedman, Trevor Hastie, and Rob Tibshirani. 2011. “Regularization Paths for Cox’s Proportional Hazards Model via Coordinate Descent.” <em>Journal of Statistical Software</em> 39 (5): 1–13. <a href="http://www.jstatsoft.org/v39/i05/" class="uri">http://www.jstatsoft.org/v39/i05/</a>.</p>
</div>
<div id="ref-ranger_pkg">
<p>Wright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” <em>Journal of Statistical Software</em> 77 (1): 1–17. <a href="https://doi.org/10.18637/jss.v077.i01" class="uri">https://doi.org/10.18637/jss.v077.i01</a>.</p>
</div>
<div id="ref-zheng2018constrained">
<p>Zheng, Wenjing, Laura Balzer, Mark J van der Laan, Maya Petersen, and SEARCH Collaboration. 2018. “Constrained Binary Classification Using Ensemble Learning: An Application to Cost-Efficient Targeted PrEP Strategies.” <em>Statistics in Medicine</em> 37 (2). Wiley Online Library: 261–79. <a href="https://doi.org/10.1002/sim.7296" class="uri">https://doi.org/10.1002/sim.7296</a>.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#introduction">Introduction</a></li>
      <li><a href="#motivation">Motivation</a></li>
      <li><a href="#area-under-the-receiver-operating-characteristics-roc-curve">Area under the receiver operating characteristics (ROC) curve</a></li>
      <li><a href="#available-wrappers">Available wrappers</a></li>
      <li><a href="#writing-wrappers">Writing wrappers</a></li>
      <li><a href="#sensitivity-constrained-rate-of-negative-prediction-scrnp">Sensitivity constrained rate of negative prediction (SCRNP)</a></li>
      <li><a href="#other-methods-implemented-in-nlpred">Other methods implemented in <code>nlpred</code></a></li>
      <li><a href="#parallelization-in-nlpred">Parallelization in <code>nlpred</code></a></li>
      <li><a href="#some-rules-of-thumb-based-on-simulation-studies">Some rules of thumb based on simulation studies</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by David Benkeser.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.3.0.</p>
</div>
      </footer>
</div>

  

  </body>
</html>
